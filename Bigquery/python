from airflow import DAG
from airflow.providers.google.cloud.operators.bigquery import BigQueryExecuteQueryOperator
from airflow.providers.google.cloud.transfers.gcs_to_gcs import GCSToGCSOperator
from datetime import datetime

default_args = {
    'owner': 'your-username',
    'start_date': datetime(2023, 1, 1),
}

with DAG('bq_org_migration', schedule_interval=None, default_args=default_args, catchup=False) as dag:

    # 元のデータセットをGCSにエクスポート
    export_dataset = BigQueryExecuteQueryOperator(
        task_id='export_dataset',
        sql='''
            EXPORT DATA OPTIONS(
                uri='gs://your-bucket/export/*.parquet',
                format='PARQUET'
            ) AS
            SELECT * FROM `your-project.your_dataset.your_table`
        ''',
    )
    
    # GCSバケットデータを新しい組織にコピー 
    copy_objects = GCSToGCSOperator(
        task_id='copy_objects',
        source_bucket='your-bucket', 
        destination_bucket='new-org-bucket',
        source_object='export/*',
    )
    
    # 新しい組織のBigQueryにインポート
    load_data = BigQueryExecuteQueryOperator(
        task_id='load_data',
        sql=f'''
            LOAD DATA
            OVERWRITE
            INTO `new_project.new_dataset.new_table`
            FROM FILES('{new_org_bucket}/export/*.parquet')
        ''',
    )
    
    export_dataset >> copy_objects >> load_data
